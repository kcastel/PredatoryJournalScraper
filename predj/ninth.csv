abstract,author,title
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>A Smart Grid (SG1 ) is an intellectual and logical electricity network that integrates the actions of all users connected to it and makes use of sophisticated and highly advanced information, control, and communications technologies to save energy, reduce expenditure and increase reliability and transparency. A smart grid can reduce energy cost; it makes energy usage efficient that result in a short term solution for the energy crisis. It also helps the distribution systems for better energy management and control. The field of Information &amp; Communication Technology (ICT4 ) and computer technology can play a major role in this hazardous situation all over the world. This paper presents current research issues and challenges that need to be addressed for reliable, efficient and flexible load distribution (LD2 ) and management for smart grid design. The paper also presents some security &amp; privacy issues that inform the grid station (GS3 ) about consumer’s habits and personnel information. The article also tries to highlight major research issues in smart grid technology, which are helpful for the new researchers to find new research directions in this field &amp; technology.</div>
		<br>
		</div>",Muhammad Zakarya,"SMART GRIDS: A prologue & unscrew challenges that needs to be addressed, A Short Survey on how to make Grids Smarter:"
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>DDoS attacks are thrown through carriage of a large amount of packets to an objective machine, using instantaneous teamwork of numerous hosts which are scattered throughout the Grid computing environment. Nowadays DDoS attacks on the Internet in general and particularly in Grid computing environment has become a visible issue in computer networks and communications. DDoS attacks are cool to provoke but their uncovering is a very problematic and grim task and therefore, an eye-catching weapon for hackers. DDoS torrents do not have familiar characteristics; therefore currently existing IDS cannot identify and discover these attacks perfectly. Correspondingly, there implementation is a puzzling task. In practice, Gossip based DDoS attacks detection apparatus are used to detect such types of attacks in computer networks, by exchanging stream of traffic over line. Gossip based techniques results in network overcrowding and have upstairs of superfluous and additional packets. Keeping the above drawbacks in mind, we have proposed a DDoS detection and prevention mechanism in [1], that has the attractiveness of being easy to adapt and more trustworthy than existing counterparts. We have introduced entropy based detection mechanism for DDoS attack detection. Our proposed solution has no overhead of extra packets, hence resulting in good QoS. Once DDoS is detected, any prevention technique can be used to prevent DDoS in Grid environment. In this paper we are going to extend our idea. A confirmation mechanism is introduced herewith.</div>
		<br>
		</div>",Muhammad Zakarya,Extended DDoS Confirmation & Attack Packet Dropping Algorithm in On-Demand Grid Computing Platform
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>Mobile Agent is a chunk of code that travels along a network from one host carrying its state of execution to another provides an execution environment. Mobile agents being programmed with decision making ability which can identify its route along a network. In this paper we present an agent based conceptual model for the railway traffic detection and management. The proposed model attempts to tackle with the bidirectional railway track conflict resolution using mobile agents for messaging and communication. The base system being the Centralized Traffic Control System, can efficiently ensures the smooth flow of traffic. The model also incorporated the accidents scenarios. In the end architectural model for both agent-owner and host are presented based on the given conceptual model.</div>
		<br>
		</div>","Muhsina Shinwari, Sher Afzal",TOWARDS THE RAILWAY TRAFFIC MANAGEMENT USING MOBILE AGENTS
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>MANETs (Mobile Ad Hoc Networks) are infrastructure-less, temporary wireless networks, consisting of several stations. No specific topology is defined in MANETs. MANETs have various applications in computer networks, such as providing communication in a domicile lacking network groundwork and proper infrastructure. In a MANET, a data packet may crisscross numerous hops until reaching its target location, making it exposed to various network attacks. The packets in a MANET are exposed to various packet dropping attacks. Mobility is there but security is the main issue still. The technology used for finding, advertising services to other nodes in the network is Service Discovery. Different Protocols are available for Service Discovery. Our focus in this paper will be on Service Discovery, available Service Discovery Architecture &amp; their modes of operation, some proposed protocols. We will discuss Mobile Service Discovery Protocol (MSDP), which have steady performance &amp; reduced massage overhead. Currently there is a diversity of service discovery protocols, most important Jini, SLP, Salutation, MSDP, Chord and UPnP. Bluetooth has also a slightly modest service discovery protocol. We have compared these tactics and listed their benefits and weaknesses.</em></div>
		<br>
		</div>","Muhammad Zakarya, Izaz ur Rahman",A Short Overview of Service Discovery Protocols for MANETS
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p>Routing protocols play an important role in the design of an efficient network. These routingprotocols include security issues i.e. false packet injections, routing table poisoning etc. In the last decade, a lot of research has been done in improving the security of these routing protocols. In this paper, we have compared Enhances Interior Gateway Routing Protocol (EIGRP) andOpen Shortest Path First(OSPF) routing protocols in secure and non-secure scenarios. Main focus is to estimate the overall behavior of routing protocols after establishing correlation among them and on the basis of convergence under secure and non-secureenvironment. The comparison result shows that OSPF has better performance in both secure and non-secure scenarios.</p></div>
		<br>
		</div>","Murad Khan, Awais Ahmad, Gissukyun Park",Computer Network Protocols Convergence Under Secure and Non Secure Environment
,,
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>This paper highlights the significance of cascade single layer models for predicting the shelf life of processed cheese stored at 7-8<sup>o</sup>C.</em><em> Mean square error, root mean square error, coefficient of determination and nash–sutcliffe coefficient were used for testing the prediction ability of the developed models. The developed cascade model with a combination of 5à8à1 showed excellent agreement between the actual and the predicted values, suggesting that single layer cascade models are efficient in predicting the shelf life of processed cheese.</em><strong></strong></div>
		<br>
		</div>","Sumit Goyal, Gyanendra Kumar Goyal",MACHINE LEARNING CASCADE ALGORITHM FOR ANALYZING SHELF LIFE OF PROCESSED CHEESE
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>Frequent changes in topology and the lack of infrastructure compel disrupted networks to avoid the use of traditional routing protocols. Rather than defining paths towards destinations, the routing tables store access chances of known nodes towards a specific destination. History of a node’s encounter is maintained in three different ways to find out its power of access to the rest of network nodes. The survey paper discusses various routing schemes based on the past encounter patterns of network nodes.</em></p><p><strong><em>Keywords</em></strong><em>:</em> (Delay Tolerant Network) DTN; routing protocols; history-based routing; frequency; encounter; inter-contact duration;  recency.</p></div>
		<br>
		</div>",MAH-RUKH FIDA,SURVEY OF HISTORY BASED ROUTING PROTOCOLS IN DELAY TOLERANT NETWORK
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Due to its novelty, flexibility and productivity the Agile Methodologies have gained much popularity and attention of the industry practitioners, designer and developers along with researchers and writers. It has become a hot topic among stakeholders over the last few years. Out of many agile methodologies Scrum has become on the top of list being a frame work. While adopting Scrum frame work, one of the scrum role i: Scrum teem have to overcome a number of problems and hurdles  related to Scrum HRM (Human Resources),  Scrum QA(Quality Assurance), Scrum Ceremonies, and Scrum artifacts  aspects of the Agile project  management in implementing Scrum. The  research paper  identified these problems using a survey and focus group discussion of experts of the field conducted in a software project company and through systematic review of  related literature .  The results and recommendations will be discussed in the paper. The results may be used by other software development companies of the area for streamlining their project management using agile methodologies</em></div>
		<br>
		</div>","MUHAMMAD ASHRAF, NATASH ALI",Impact Agile Project Management:  Identification And Analysis Of Problems In Scrum Implementation
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>The fields like Software Engineering (SE) and Human Computer Interaction (HCI) are considered dissimilar.. SE based process model mostly discuss modeling of functional requirement while the HCI based approaches are mostly concerned with the modeling of quality attributes. The quality attributes are mostly discussed during late phases of software development. The non-functional requirements as quality attributes can be integrated in software products by considering quality or non-functional modeling approaches during all of the phases of software engineering process model. The separation of SE and HCI concerns restricts formal specification of quality attributes during all of the phases of SE process model. The software systems or products are generally less user centered because SE process models can’t address formal specification of quality attributes in SE process models. In this research a methodology for the formal specification of approaches that model functional requirements and quality attribute during SE process model is proposed. The proposed methodology is based on waterfall SE process model. It can be utilized in design and development of users centered software products. Our proposed methodology also bridges gap between SE and HCI fields.</em></p></div>
		<br>
		</div>",UMER RASHID,A Methodological Approach: Formal Specification Of Quality Attributes Modeling Approcahes In The Waterfall Process Model
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>The Wireless Sensor Network (WSN) is an emerging field of wireless network comprising of few to many autonomous tiny sensors nodes, with limited processing, limited memory, limited battery power, limited bandwidth and limited wireless transmission capabilities. The life time of the sensor node depends upon the battery power. WSN are commonly used to monitor environmental conditions like temperature, sound and pressure etc. WSN is an application of MANET. Wireless sensor node collects data and sends back to the sink or Base Station (BS). Data transmission is normally multi-hop among sensor nodes that enable these nodes to transmit data from hop to hop towards the sink or BS. Wireless sensor network requires robust and energy efficient communication protocols to minimize the energy consumption as much as possible.  Main penalty area of researchers is to design the energy efficient routing protocol. Routing protocols should be energy efficient, scalable and prolong the network lifetime.But Quality of Service QoS is also a challenge for energy efficient routing protocols for researchers. QoSneeds a multi-layerlinespanning using the different layer protocol architecture. In this paper, we enlighten the energy efficient routing towards QoS in WSNs and proposes a solutionfor the QoS layer in energy efficient routing techniques in WSNs and finally, highlight some open problems and future direction of research for given that QoS in WSNs.</em></p></div>
		<br>
		</div>","SHAKEEL AHMAD, ASIM SHAHZAD, NATASH ALI",A Comprehensive Study Of Energy Efficient Routing In Wsn Towards Qos
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Different methodologies have been used to design and model a system which having the ability to make decision in uncertain and indecisive situation. In this paper we present the comparisons of membership and reduced membership functions for fuzzy rules. A model of different fuzzy rules is designed for three membership functions (mf) and then reduced the fuzzy rules by reducing the mf. </em></div>
		<br>
		</div>","SHAH NAZIR, MUHAMMAD NAZIR",Comparisons Of Membership Functions For Fuzzy Rules
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>Memories are the internal mental records that we maintain .Human mind is a very complex organ.. Processing depends on how we memorize information, events and how we recall things and use them efficiently in situations when required. It can be related that for Storage in mind we use different data structures for storing variety of information. We remember the names of known persons, and the people we met more frequently.The Topics in book, Months of the year, our CNIC Number, the way we learn words of a new language etc. Recently invented data structures e.g skiplist [1] show much similarity of how the brain store the information. So we can say Careful study of how the cognitive storage works could lead to the discovery of the new data structures In this paper we have attempted to relate the existing data structures with how we store information in mind.</em></p></div>
		<br>
		</div>","ABDUL WAHAB, ADNAN ABID, AMJAD IQBAL",Cognitive Storage Model And Mapping  With Classical Data Structures
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>When we think about formal method; the first thing which comes in our mind is mathematical approach. The process of formalization is an approach based on mathematics and used to elaborate the properties of systems (hardware and software). The mathematical modeling or formal methods provide us a framework for large and complex systems. Thus these systems can be specified, analyzed, designed, and verified in a systematic way rather than the approaches which are used conventionally. Formal verification and the methods are applied using theoretical computer science fundamentals to solve the complex and difficult problems in large and complex software and hardware systems to ensure the systems will not fail with run-time errors. Conventional approaches of software verification in call distribution systems rely on quality assurance to verify the system behavior and robustness. The process of software testing cannot show the absence of errors it can only show the presence of errors in software systems. [1] In contrast, the mathematically-based techniques of verification are based on formal methods to prove certain software attributes, for example proving that software does or does not contain the occurrence of errors at run-time such as overflows, divide-by-zero, and access violation, invalid memory access and stack/heap corruption. [1] In this paper later we will have comparative analysis of formal methods vs. conventional software development approaches in call distribution systems. Using this comparison we‘ll try to identify the methodologies and approaches which would be better in SDLC for call distribution systems. </em></div>
		<br>
		</div>","MUHAMMAD AJMAL SIDDIQUI, MUHAMMAD SALEEM AKHTER, NATASH ALI MIAN",A Comparative Analysis Of Conventional Software Development Approaches Vs. Formal Methods In Call Distribution Systems
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Dengue fever is widespread disease in the tropical areas caused by bite of female Eddie mosquito. Pakistan has been victim of this rapidly growing disease since last few years. The world health organization identified the four types of dengue fever. The experts are facing the problem of misdiagnosis of dengue fever. The Tests needed for empirical Classification of Dengue Fever takes a lot of time and money especiallyin epidemic situation in a country with limited resources. Therefore, we have used data mining techniques for the efficient classification of the dengue fever Type. The decision tree learning algorithm has been used as a classification Model. We performed two experiments using decision tree. The first general experiment shows the accuracy of 99.44%. It prunes the attributes which classify the dengue fever on the basis of the values in the dataset. The Second experiment classifies the dengue fever on the basis of expert weighted attributes, which are used in the classification on the basis of Minimum Cost and resource availability. The accuracy of this Model is still high 98.62%. We compared the performance in term of Type II error. We found that the Type II error is very low in second experiment</em></div>
		<br>
		</div>","WAJEEHA FAROOQI, SADAF ALI, ABDUL WAHAB",Classification Of Dengue Fever Using  Decision Tree
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>Almost all the organizations are seriously thinking to adopt the cloud computingservices, seeing its benefits in terms of cost, accessibility, availability, flexibility andhighly automated process of updation. Cloud Computing enhance the current capabilitiesdynamically without further investment. Cloud Computing is a band of resources, applicationsand services. In cloud computing customer’s access IT related services in terms of infrastructure platform and software without getting knowledge of underlying technologies. With the executionof cloud computing, organizations have strong concerns about the security of their data.Organizations are hesitating to take initiatives in the deployment of their businesses due to data security problem. This paper gives an overview of cloud computing and analysis of security issues in cloud computing.</em></p></div>
		<br>
		</div>","AKRAM MUJAHID, TARIQ MAHMOOD, WASEEM IQBAL",Comparative Analysis Of Cloud Computing Security Issues
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Mobile ad-hoc networks (MANETs) are temporary networks without any pre-existing infrastructure in which nodes are connected through wireless channels. There is no centralized administration in this network because nodes are not fixed at their locations, they are continuously moving to different locations. Each node in MANETs has dynamic capabilities because at one point acts as a host while in some other time becomes a router which can forward messages on the network to other nodes. One of the complicated issues in MANETs is routing due to their dynamic topology which is frequently changes due to the mobility of nodes. In order to forward packets in this complex topology using optimal routes, there is a need for efficient routing protocols. There are three categories of routing protocols in MANETs named as reactive, proactive and hybrid respectively. We have used a hybrid type protocol named Zone Routing Protocol (ZRP) for routing in our research. In order to control unwanted flooding in the overlapping zones of ZRP, we have proposed a selective broadcasting technique for route discovery. In this technique route discovery exchange messages are highly reduced in the outer zone of the ZRP.</em></div>
		<br>
		</div>","ISHAQ AHMAD, Omar M. Barukab, Sher Afzal Khan",Reducing Flooding of Zone Routing Protocol in Mobile Ad-Hoc Networks
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>Among various software development models, the focus of agile models is disciplined management projects, an approach that enforces self association, collaboration and liability. Agile models adopt a business alignment that supports development with chump needs and aggregation goals. Extreme Programming (XP) and Scrum are often used models of agile whereas Rapid Application Development (RAD) is a conventional plan driven software development model. The purpose of introducing RAD is to include functionality to an application.  Strengths of Scrum and RAD are that they are self managed processes through iterative planning. Basically RAD is the advanced version of XP that’s why XP is also included just for reference.  This research work is intended to analyze the strengths, characteristics and weaknesses of Scrum, RAD and RAD models. The paper also explains the disciplines and phases of RAD that can enhance the robustness of RAD and Scrum models. It will also propose a narrative hybrid model that combines RAD, Scrum and RAD to strengthen their features and removing their weaknesses.</em></p></div>
		<br>
		</div>",Qurat -ul- Ain,A Hybrid Model By Integrating Scrum And RAD
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>New trend for achieving high performance in processors history as well as evolutionary change in the world of IT has revolutionized the concept of speed and performance in industry .The best way to enhance performance is the way to add multiple cores.  To take full advantage of Multi-core processors require some means of dividing work among the cores. It’s up to industry to overcome the problem of programming and hardware to take the full advantage of multi cores. With the passage of time, Pc’s have become rifer and a large number of applications have been designed for the PC’s and end-users and application programmers need more faster and more powerful s</em><em>ystem capable of performing by </em><em>trillions of instruction with low price and high performance. A number of factors have achieved speed by increasing clock speed, adding multiple cores but on the same chip. But here the time is not on end all the manufacturers moves upon to use multiple cores. Multi core simulations helped to understand the communication overhead.</em><em></em></p><p><em>This paper will describe how far industry has progressed and evaluates some challenges that industry faces with multi-core processors. The focus of this paper is on defining a relationship between performance and power level of </em><em>m</em><em>ulticore processors and single core processors</em><em> by defining complete history of processors and what the reason behind the innovation of multi-cores was? How they help to achieve highest performance?</em><em></em></p></div>
		<br>
		</div>",Qurat -ul- Ain,Search for Excellence & Performance -Multicore Processors
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>In lossless compression techniques, perfectly identical copy of the original image can be reconstructed from the compressed image. The paper implements three lossless compression techniques namely Huffman Encoding, Run Length Encoding and DPCM techniques using MPI. The experimental results show considerable reduction in execution time and better compression ratio for certain types of images.</div>
		<br>
		</div>","Hanif Durad, Waqas Kazmi, Muhammad Naveed Akhtar",PARALLEL LOSSLESS IMAGE COMPRESSION USING MPI
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>MPLS has a choice of two signalling protocols CR-LDP and RSVP-TE. Both protocols have the ability to provide QoS, constraint based routing, explicit routing and traffic <br>engineering in the core network. In this paper both signalling protocols performance is analyzed and its conclusion helps ISP and carrier providers to make a better choice of signalling protocol as per their needs. The paper reviews the pros and cons of both signalling protocols and then compares the CR-LDP and RVSP-TE in term of bandwidth and throughput of a link for a small and large scale network using video traffic.</div>
		<br>
		</div>",Ghani Rehaman,SCALABLITY ANALYSIS OF MPLS LABEL DISTRIBUTION PROTOCOLS RSVP
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>This paper is using byte oriented encryption scheme for securing HTTP traffic to take less time as compared to HTTPS which is using bib by bit encryption scheme. The resulted data show that it take less time as compared to HTTPS and is more secure as compared to HTTP.</div>
		<br>
		</div>",Zahid Haroon,Securing HTTP traffic using byte oriented encryption scheme
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>In the present era abundant web portals are available over the internet. In the proposed work we concentrate on data mining of Quranic web portals. To know and obtain awareness about Islam, numerous Quranic web portals are being accessed worldwide. Data mining is one of the emerging technologies which analyzes raw data using supervised and unsupervised techniques to find the hidden pattens. This paper is intended to study the access pattern of some of these websites region wise using classification based data mining under which ROC plots have been depicted. The AUC of depicted ROC of considered Islamic web portals are obtained and have been distinguished as to which portal’s prediction is more appropriate. Alexa’s web-site is an effective tool for obtaining the required data about each of these Quranic web portals regions wise. The study is focused to analyze this data and find the reasons for certain preferences.</div>
		<br>
		</div>",Mohammad Khubeb Siddiqui,A REVIEW of QURANIC WEB PORTALS THROUGH DATA MINING
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>Web services are the emerging technology used to perform complex tasks. Web services are available over the internet and different application can request a specific service to perform the required tasks. As web services are used by different type of applications having different models and protocols, maintaining the interoperability are very important. In this paper we consider the different techniques, standards and models used to maintain interoperability, i.e. WSDL-S, SOAP engine and UDDI (Universal Description Discovery and Integration)[5]. As web services are used commercially then main issue is the security. Integration layer Model is available that is used in support of interoperability in web services [1]. In this model, layers are divided in to low level (with more interaction) to high level (with less interaction). Main purpose of this paper is to propose the new enhanced based model. This model is based upon the layer architecture, to support message passing between services having different syntax, semantic and underlying platforms.</div>
		<br>
		</div>","Shahazada Zeeshan Waheed, Farooq Azam",Enhanced Layer Based Model in support of Web services Interoperability
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>Nanotechnology have many potentials to be used in various research directions for humanity. It is being used in many applications and products nowadays. One of the important application directions for this emerging technology is biomedical science and human body diagnosis. Nanomachine technology is part of the ﬁeld concerning about creating and maintaining Nanoscale machines capable of performing certain functionalities such as molecule counting [1], molecule shuttling [2] and molecule sensing [3]. Communication between Nanomachines adds more capabilities and allows cooperative and distributed functionalities and form the concept of NanoNetworks [4]. Today’s researchers are trying their best to deploy Nanomachines and NanoNetworks for real-time objectives such as medical purposes. Moreover, the same problem is faced in other ﬁelds such as battleﬁeld networks, environmental monitoring, and automation ﬁelds. However, due to design issues of Nanomachines such as low computation capability, low processing power, limited storage, imperfect sensing, actuation, and limited networking capabilities, the work done in the ﬁeld of Nanonetwork communication is inadequate. <br>In this paper, we present a novel communication protocol stack model for Nanomachines and show how this model can be used in medical applications for human body diagnoses. Our model provides guidance regarding designing protocol stacks for Nanomachines.</div>
		<br>
		</div>","Fasee Ullah, Imran Khan",BNMPS: BIOMOLECULAR NANOMACHINE PROTOCOL STACK FOR HUMAN DISEASE DIAGNOSES: A NEW PARADIGM
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>Ontology is an efficient and an adequate way of conceptualization of knowledge representation for real world applications. Ontologies are successfully used in varieties of knowledge domains. <br>In this paper, we focus on the field of medical ontology knowledge construction and representation. We identified three different medical ontologies classes: Generic, Specific and Mass Casualties Incidents (MCI) medical ontologies. This survey should support the construction of medical Ontologies in future for a better patient treatment. In particular, we survey and focus on the existing medical related ontologies and investigate their suitability for use in Mass Casualties Incidents (MCI). </div>
		<br>
		</div>","Fasee Ullah, Imran Khan",USING ONTOLOGIES FOR THE EMERGENCY RESPONSE SUPPORT: A SURVEY
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>Learning objects are reusable units of learning that are used for educational purpose. E-learning has now become quite prevalent in learning community, more specifically in the higher education. Teachers and learners play crucial role in the establishment of e-learning community but the system is still not freed of the traditional barriers of the conventional mode of education for the provision of learning contents. The educators and instructional designers are still working on confined repositories for the purpose of authoring learning objects. The learner’s context and behavioral patterns are still not properly integrated in the e-learning system .The study aims to propose architecture for e-learners to get personalized contents in an adaptive fashion. The initial purpose of this study is to perform a quantitative analysis of learning content. The study also contributes to the proper placement of Learning Objects Repository (L.O.R) in the proposed framework and the integration of various tools at the front layer. However the most important contribution of the research is to propose an algorithm that can rank LOs for a learner with reference to his pedagogical needs and learning goals.</div>
		<br>
		</div>","Shaina Qamar, Syed Raza Bashir",TOWARDS THE RECOMMENDATION OF HIGHLY RELEVANT LEARNING OBJECTS TO THE LEARNERS
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>The technology of Ultra Wide band now a days is quite demanding due to the characteristics like its simple architecture , low power consumption and cost reliability but still it faces some deficiencies in term of its design to achieve low complexity and low cost. UWB systems experience problems while using digital signal processing technology and require high sampling frequencies. In this paper, the performance of UWB system in the cooperative communication environment is evaluated in terms of its Bit Error Rate for different number of relays and different average distances from source to destination node. The simulations are performed for both line of sight (LOS) and non-line of sight (NLOS) environment. Results from simulation shows that the performance of the system decreases by increasing average source to destination distance. The simulation results also shows that the system performs better in LOS channel environment as compared to NLOS channel environment. In the end results, it shows that the performance of the system increases by increasing the number of relay nodes to adequately large number.</em></div>
		<br>
		</div>","Pir Meher Ali Shah, Latif Jan, Muhammad Amin, Salman Khan",Code Shifted Reference Based Cooperative Using Multiple Relays In Ultra Wide Band Communication System
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Assembly lines have very important role in manufacturing. These are used to manufacture large scale series of products. Developments over the period of time changed the assembly lines from single model lines to more convenient systems with some variations including two assembly lines, ‘n’ assembly lines, customer oriented mixed model, multi model and u shaped lines and many more. In this paper attention is focused to develop the model which is used to assemble the three different types of autos which have some common auto parts. The main objective is to design this model to reduce the time when the different types of autos having some common auto parts are assembled. The optimization problem in this paper is solved by using dynamic programming approach. For this purpose six assembly lines are used having ‘n’ number of stations, two assembly lines are specified for each type of auto. Assembly lines are set in such a way that the common auto part of three types must be assembled at the parallel station of the lines</em></div>
		<br>
		</div>","Zaheer Ahmad, Shahid Yousaf",Manufacturer Oriented Mixed Model Assembly Line Scheduling using Dynamic Programming
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>Visual representations are always better than narrations in accordance to children, for better understanding. This is quite advantageous in learning school lessons and it eventually helps in engaging the children and enhancing their imaginative skills. Using natural language processing techniques and along the computer graphics it is possible to bridge the gap between these two individual fields, it will not only eliminate the existing manual labor involved instead it can also give rise to efficient and effective system frameworks that can form a foundation for complex applications. In this paper we present an architecture to design for a NLP engine that can be used for 3D scene generation, the input would be in textual form that would be processed by each module of the natural language processing (NLP) engine. This text would be restricted in terms of the constraint based grammar (CBG), eliminating the maximum occurrence of any ambiguity and easing the noun fragmentation process. Eventually, the output of the NLP engine would be a sentence that fulfills the custom grammatical rules.</em></p></div>
		<br>
		</div>","NABEEL SABIR KHAN, Adnan Abid",CONSTRAINT BASED NLP ENGINE
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>This paper is aimed at news classification on basis of their headlines. Researchers have worked a lot for carrying out news classification at full text level but work in the domain of news headlines classification exists in very limited ratio, Therefore, after analyzing variety of existing news classification methodologies, a probabilistic framework is presented in this paper for classifying news headlines. News headlines classification process is divided into three modules, headlines pre-processing module, probability learning module, and news headlines classification module. Based on availability of variety of headlines, probabilistic framework is designed, which classifies each news headline to its pre-defined category by calculating its maximum probability in that category. Work has been performed using bag of words approach where each headline is split into words and each word is given a certain probability. Furthermore, it is shown that proposed system gives better accuracy results as compared to existing headline classification systems.</em><em></em></p></div>
		<br>
		</div>","Mazhar Iqbal Rana, DR. Shehzad Khalid, Fizza Abid, Armugh Ali, Mehr Yahya Durrani, Farhan Aadil",News Headlines Classification Using Probabilistic Approach
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Texture analysis is considered fundamental and important in the fields of pattern recognition, computer vision and image processing. Analysis of the textures involves texture features extraction and selection, and plays an essential role in the classification and segmentation of textural features. In this study have compared two texture classification methods based on the Random Forest (RF) and Decision Tree (DT) classifiers by using a combination method between various extraction features, such as bi-orthogonal wavelet transform, gray level histogram and edge detection. Experiments were conducted on two different databases. The first texture database captured digital images for testing multi-class machining processes, and the second database was collected from the Brodatz album. The results have revealed that RF and DT have yielded higher classification precision.</em></div>
		<br>
		</div>",mohammed majeed razooq,Atexture Classification Using Random Forest And Decision Tree
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>The popularity of java programming is growing continuously, as Java gives an independent, object oriented and multithreaded programming environment.</em><em>[1]</em><em> J2ME (Java 2 Micro Edition) is the Most Ubiquitous Application Platform for Mobile Devices and provides a robust flexible environment for all application running on cell phones and also other embedded devices</em><em>[11]</em><em>[12]</em><em>. </em><em>In this paper an application is develop for mobile phone and PC (Personal Computer), using Java programming language. The main function of this application is to transfer images automatically from mobile phone to PC or laptop and also save these images in computer system memory (Hard Disk), whenever mobile phone detect PC or laptop in its Bluetooth range. The mobile phone app save the PC name in its database as server and sends the images to only that PC while the rest of the PC in its Bluetooth range are ignored plus the mobile app also save the sent images name list so that the images are not send twice to PC. This application has been tested on student’s mobile phones in the university premises and shows a promising results and will also be tested on public mobile devices in future studies.</em></p></div>
		<br>
		</div>","Salman Khan, Hashim Ali, Salman Saleem",Research On Auto Mobile-Pc Upload  Images Application Through  Bluetooth Using Java
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>Graph coloring issue, is one of satisfied existing constraints issues in the literature of artificial intelligence. Coloration apical includes assigning color to node graph so that any two adjacent vertices are isochromatic. The minimum number (colors numbers) that we assign to these graphs for coloring are called number of color. This issue is from the group of very difficult issue, NP – complete. Given the importance of graph coloring issue and its many uses, many algorithms suggested finding allowed coloration in graph. Among these can be noted in, exact algorithms, distributed algorithms, parallel algorithms, approximation algorithms and heuristic algorithms, …The concept of learning Automata at first was introduced by Tstlyn. He was interested in modeling the behavior of biological systems, and definite automata worked in a random environment, introduced as a model for learning. The aim of this research is to present new algorithm on the basis of learning automata to color with accuracy and high speed and the ability to learn graph vertices. The proposed method also has transfer chart and individual performance and this method was examined on the graph with low vertices and high vertices and medium vertices, on the bottom, a number of works steps and total dyes used for coloring of specific graph with optimization algorithms were matched. Evaluation results show high accuracy, speed and its performance of the proposed method is superior to other optimization methods. </em></p></div>
		<br>
		</div>","Susan Khorramdel, Homayoun Motameni, Farhad Ramezani",The Design Of New Learning Automata For Problem Graph Coloring
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Despite the widespread use of online applications and websites, inadequate attention is paid to their usability aspect. Always there is a need to evaluate the usability of these applications and website specially the educational one to improve their user friendliness. The idea behind this research is to evaluate the usability of an educational website in Saudi Arabia. For this research we have considered a number of ways of usability evaluation, but find user based method [7] specifically Heuristic evaluation method to be easier and more cost-effective way to evaluate the usability of educational websites. A questionnaire was developed to accumulate the data. Then survey is conducted based on this questionnaire, by providing this questionnaire to different undergraduate students in one of the universities in Saudi Arabia.</em><em>Despite the widespread use of online applications and websites, inadequate attention is paid to their usability aspect. Always there is a need to evaluate the usability of these applications and website specially the educational one to improve their user friendliness. The idea behind this research is to evaluate the usability of an educational website in Saudi Arabia. For this research we have considered a number of ways of usability evaluation, but find user based method [7] specifically Heuristic evaluation method to be easier and more cost effective way to evaluate the usability of educational websites. A questionnaire was developed to accumulate the data. Then survey is conducted based on this questionnaire, by providing this questionnaire to different undergraduate students in one of the universities in Saudi Arabia.</em></div>
		<br>
		</div>","Hina Gul, sardar zafar Iqbal, Madeeha Saqib",Usability Evaluation of an Educational Website in Saudi Arabia
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Reliable communication between transmitter and receiver is accomplished by cooperative diversity techniques. Sending of data in various paths has greatly improved the performance of communication. We have studied the performance of Amplify-and-Forward (AF) based network in this work for diverse relay location at Nakagami, Rician and Rayleigh fading channels. The relay performance in Amplify-and-Forward (AF) protocol based on Symbol Error Rate (SER) against Signal-to-Noise Ratio (SNR) in dBs is calculated. The software that is used to construct Monte-Carlo link level simulation is MATLAB. The effects of a relay at changed location in diverse channels accompanied with Additive White Gaussian noise (AWGN) is also calculated. BPSK modulation scheme is used for the transfer of information between the source, relay and destination node. The signals are combined through Maximum Ratio Combining method (MRC).</em><em>Reliable communication between transmitter and receiver is accomplished by cooperative diversity techniques. Sending of data in various paths have greatly improved the performance of communication. We have studied the performance of Amplify-and-Forward (AF) based network in this work for diverse relay location at Nakagami, Rician and Rayleigh fading channels. The relay performance in Amplify-and-Forward (AF) protocol based on Symbol Error Rate (SER) against Signal-to-Noise Ratio (SNR) in dBs is calculated. The software that is used to construct Monte-Carlo link level simulation is MATLAB. The effects of relay at changed location in diverse channels accompanied with Additive White Gaussian noise (AWGN) is also calculated. BPSK modulation scheme is used for the transfer of information between the source, relay and destination node. The signals are combined through Maximum Ratio Combining method (MRC).</em></div>
		<br>
		</div>","Salman Saleem, Humaira Rehman, Imran Khan et.al.,","The SER Analysis of Rayleigh, Rician and Nakagami Channels at Various Relay Locations in Cooperative Networks"
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>The number of vehicles/traffic is increasing exponentially day-by-day, due to which automatic vehicles identification/monitoring system is being paid significance all over the world. Different countries in the world are using various types of automatic systems for traffic control, vehicles identification.  Number plate recognition (NPR) is an authentic-time embedded system which frequently recognition the number plate of vehicle.  Pervious systems are using only for identification of vehicle. The proposed system cumulates both the RFID (Radio Frequency Identification) and NPR systems for the identification and verification of vehicle.</em></div>
		<br>
		</div>","Aqib Mehmood Durani, Mumtaz Ali, Rameez Ahmad, Syed Irfan, Habib ur Rehman",Identification and Verification of Vehicle using RFID Technique
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>Wireless communication networks using unlicensed frequency band faces certain challenges like unrestrained interfering and bad quality of transmission. To surmount the scarcity of frequency band, a new technique for wireless communication is compulsory to adapt the exponentially rising wireless communication demand. Visible light communication systems (VLCS) offer a replacement to the existing standards of wireless communication, through light from light-emitting diodes (LEDs) as the mean of communication. As LEDs twinkle repeatedly at a high speed such that human eye cannot perceive changes in light intensity, but a perceptive photodiode detect the on-off attitude and can interpret the data implanted within the light. This paper investigates different issues in the existing wireless communication networks, and studies how VLCS can resolve these issues, and proposes design of the VLCS. Moreover, applications, solution to current issues and future improvements are discussed in this paper.</em></p></div>
		<br>
		</div>","Fazlullah Khan, Farman Khan, Qamar Jabeen, Syed Roohullah Jan, Shehzad Khan","Applications, Limitations, and Improvements in Visible Light Communication Systems"
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>We present a humans credentials system centered on ambulation characteristics. This problem is as eminent as acoustic gait recognition. The objective of the scheme is to explore sounds radiated by walking persons (largely the musical note sounds) and identifies those folks. A cyclic model topology is engaged to denote individual gait cycles. This topology permits modeling and detecting individual steps, leading to very favorable identification rates.</em></div>
		<br>
		</div>","ASADULLAH KHAN, Farooq Ahmad, YASER DAANIAL KHAN, DAOJING HE, Mudasser Naseer",GAIT RECOGNITION PROGRESS IN RECOGNIZING IMAGE CHARACTERISTICS
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Smartphones have become a potential part of our lives, and this led to a continued increase in the number of smartphone users. The growing number of users attracts hackers to develop malware applications to steal the private information and causing potential financial losses. Due to the fast modifications in the technologies used by malware developers, there is an urgent need for more advanced techniques for malware detection. In this paper, we propose an approach for Android malware classification based on features selection and classification algorithms. The proposed approach uses the permissions used in the Android app as features, to differentiate between the malware apps and goodware apps. The information gain algorithm is used to select the most significant permissions, then the classification algorithms NaivBayes, Random Forest and J48 used to classify the Android apps as goodware or malware apps. The experimental results show that random forest algorithm achieved the highest precision of 0.898 with a lowest false positive rate of 0.110.</em></div>
		<br>
		</div>",Altyeb Altaher,Classification of Android Malware Applications using Feature Selection and Classification Algorithms
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>This paper present the new idea of Vertical handover for heterogeneous Wireless Networks, based on different parameters values i.e, available Bandwidth, received signal strength, mean number of request per unit time, mean number of calls served per unit time, power deception, power consumption, network condition, duration for mobile station to present in network .Three network selected for the handover, the vertical handover is take place at best resulted network between the available networks. In this article, provoked with facts and figure which done by vertical handoff procedure on mobile nodes But the crucial parameters which is battery power may also consider highly for certain mobile nodes, call blocking probability of networks are define to make decision for network with good quality, for the proposed algorithm experiment results are shown between the available networks and parameters, graphically illustrate the vertical handover to the specific network.</div>
		<br>
		</div>","Zakir Ullah, Shazada Alamgir, Alamgir Khan, Salman Saleem, Imran Khan",Efficient Algorithm for Vertical Handover in Heterogeneous Wireless Mobile Network
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>The Web contributes greatly to our life in many fields such as education, entertainment, Internet banking, online shopping and software downloading. This has led to rapid growth in the number of Internet users, which resulting in an explosive increase in traffic or bottleneck over the Internet performance. This paper proposes a new approach to group users according to their Web access patterns. The proposed approach for grouping users is based on Fuzzy c-means technique, which allows web users to be assigned into more than one cluster or interest. Each web user has a degree of membership of belonging to each cluster. The experimental results showed that the web users were successfully clustered to similar groups very fast using Fuzzy-c-means.  In addition, the Fuzzy-c-means performed well and became much better when the clusters number increased on two real Bo2 and NY datasets. The proposed intelligent web users clustering based on Fuzzy-c-means can be used for discovering users' interests in Web pages that can contribute in enhancing several approaches such as Web caching, Web pre-fetching and Web recommender systems that are recently used to improve the Web performance.</em></div>
		<br>
		</div>","Waleed Ali, Mohammed Alrabighi",Web Users Clustering Based on Fuzzy C-MEANS
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>There has been an explosive growth in medical imaging industry with rapid development in imaging techniques. Magnetic resonance imaging (MRI) has increasingly been popular as a result of its significant role in examining the ever-changing activity of human brain. The image and resolution produced by MRI is quite detailed and can detect tiny changes of structures within the brain and human body at large. It has become the leading technology for examining the living brain at work. More importantly, it plays an extremely important role in diagnosing patient with Alzheimer’s disease in its early stage. In fact, this will dramatically assist in analyzing the brain scan images and identifying whether if there is any potential risk for developing Alzheimer’s disease. Quantifying the brain for Alzheimer’s patient on regular basis is very important so that to critically analyze and assess the rate at which the disease affect patient brain. However, the quality of the MRI scans can radically change due to environmental and atmospheric changes, and equipment ageing. Subsequently, this has a profound impact on the precision and accuracy of the measurement of the brain. In this paper, we proposes an approach which eliminates the in homogeneity problem anticipated so that 3D brain MRI scans can be efficiently processed and analyzed</em></div>
		<br>
		</div>","Yakubu Suleiman Baguda, Abubakar Suleiman Baguda, Usman Suleiman Baguda",Intensity Inhomogeneity Correction Scheme for 3d-Dimensional Mri Brain Scans using Histogram Matching
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p>The implications of specific biochemical and structural features of many putative virulence factors in the pathogenesis of MAP in ruminant and human being are relatively less explored and understood. In this background, the comparative genomic and proteomic studies of MAP_PRSO3010, a putative protein shows that it shares a sequential, physiochemical, structural and functional similarity with Rv0757 (PhoP) protein, a virulent molecular determinant of Mycobacterium tuberculosis (Mtb). Conserved Domain Database (CDD) and InterPro studies demonstrate the presence of conserved domains namely Signal transduction response regulator, receiver domain (REC) and Transcription regulatory protein, C-terminal (Trans_reg_C) belonging to CheY-like and Helix Turn Helix (HTH) superfamily, respectively in both MAP_PRSO3010  and Rv0757 proteins. These domains are mainly involved in phosphorelay signal transduction and transcriptional regulation in response to unfavorable environments within host macrophages. Comparative in silico protein-protein interaction (PPI) studies also showed the involvement of common interacting proteins namely mtrA/B crucial for the survival of Mtb within host macrophages. The predicted hypothetical model of MAP_PRSO3010 protein provides an insight on the functional and structural resemblance between the two proteins. The model quality and structure assessment tools of Swiss-Model Server also validated the predicted hypothetical structure of MAP_PRSO3010 protein. Thus, these results show a strong relevance of MAP_PRSO3010 protein in MAP virulence</p></div>
		<br>
		</div>",Syed Asif Hassan,Comparative Computational Analysis of a Putative Transcriptional Regulator Map_PRSO3010 and its implications in the Pathogenesis of Crohn’s and Johne’s diseases
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p>. Evolution of multi-drug resistance strains of <em>Mycobacterium tuberculosis</em> (MTB) has in the past caused severe epidemics of tuberculosis across the globe thereby it raises a question on the potency or efficacy of the current scaffolds of drugs targeting MTB. Therefore, there an urgency to identify novel antimycobacterial compounds targeting proteins important for the viability of <em>Multi-Drug-Resistant Tuberculosis (MDR-TB) strains. In this regard Mtb LprG (Rv1411c) a lipoprotein involved in the evasion of cell-mediated immune response within infected host macrophages is an important target for screening antimycobacterial compounds against Mtb. In the current study, a workflow involving ligand-based virtual screening namely USRCAT (Ultra Shape Recognition) and molecular docking studies were employed to identify novel antituberculosis compounds. Based on USRCAT and docking studies XPX an analog of triacylated glycolipid was screened as a promising lead molecule that shows higher specificity and binding affinity for Mtb LprG protein. Further, in vitro experimentations are required to testify the role of XPX as an anti-TB drug for the treatment of MDT-TB.  </em></p><p><em> </em></p><p><strong><em>Keywords</em></strong><em>:</em> Multi-drug Resistant Tuberculosis; Toll-like receptor 2 (TLR2); antagonist; Molecular docking; Virtual screening</p></div>
		<br>
		</div>","Syed Asif Hassan, Tabrej Khan, Arshad Hashmi",Computational Approach to Design Antagonists of Mycobacterium Tuberculosis Lipoprotein Lprg (RV1411C) Protein
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>Nowadays, most of the academic institutes facing a low-quality problem in the educational field. One of these factors is an educational student achievement and staff teaching quality. This study presents an efficient framework for assessment and prediction of teachers’ performance in academic institutes using Artificial Neural Network (ANN) algorithm. The proposed framework designed to predict the performance quality level of teachers in order to improve the learning outcomes. The prediction model was tested effectively using the TA UCI dataset. The data consists of academic experiences for teachers as well as their experiences and grades of students in courses they taught among others. The SPSS tool was used to build the suggested prediction system. The TA data was dividing into three groups (70, 80, and 90) for training data, and (30, 20, and 10) for testing data respectively to study the dataset discrimination. The results are showing that the neural network obtained better accuracy results with (90%) in the training and (10%) in testing.</div>
		<br>
		</div>",Ahmed Hamza Osman,An Evaluation Model of Tecaching Assistant using Artificial Neural Network
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p>Abstract. The metalloenzyme peptide deformylase (PDF) catalyzes the elimination of the N-formyl group from N-terminal methionine upon translation, which is crucial for protein synthesis, growth, and survival of bacteria. In this context, we aim to identify potent derivatives of the known mycobacterial PDF (mPDF) inhibitors having better pharmacological properties than their parent compounds. Initially, BB-83698, Galardin, and LBK-611 known mPDF inhibitors were selected based on their binding affinity for mPDF using iGEMDOCK. Analogs of these three inhibitors were prepared. Further, the analogs were screened based on their oral bioavailability, pharmacokinetics properties, drug likeliness and binding energy. The post-screening analysis reveals that the analog, (2R)-N’-hydroxy-N-[(2S)-3-(5H-indol-3-yl)-1-oxopropan-2-yl]-2-(2-methylpropyl) butanediamide (CID5288446) of galardin interacts with residues GLN56, LEU107, HIS148, GLU149, and HIS152 near the vicinity of the active site (H<sup>132</sup>EXXH<sup>136</sup>) of mPDF protein with higher affinity as compared to its parent compound galardin. The prediction tool based upon structure-activity relationship reveals that the analog CID5288446 showed similar metalloproteinase activity with lesser toxic effects when compared to its parent compound galardin.</p><p><strong>Keywords:</strong> Mycobacterial Peptide Deformylase (mPDF); structure-based virtual screening; fitness score; pharmacokinetics properties; drug likeliness; Inhibitors.</p></div>
		<br>
		</div>","Syed Asif Hassan, Iftekhar Aslam Tayubi",Computational Approaches to Identify a Derivative of Galardin as an Inhibitor of Mycobacterial Peptide Deformylase
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><strong>Objectives:</strong> To increase needed for exchanging digital photos electronically, due to alarming demand for multimedia applications, and because of the increasing use of images in electronic processes. Hence, the need for protection by unauthorized user is necessary. <strong>Method:</strong> This paper primarily is focusing on the necessary protection of these images using a specific analyzes algorithm: Advanced Encryption Standard (AES) with a full its description, which is known as an algorithm (Rijndael). <strong>Findings:</strong> It will be determined the address decryption, which is made up of different styles in all encryption and decryption steps in order to protect the valuable information. This algorithm will be implemented on MATLAB software programming. <strong>Application:</strong> The above results and analysis for this crypto system based on AES algorithm give a high performance. So we have reason to believe that use this method to encrypt the image will have a very good prospect in the future.</p></div>
		<br>
		</div>",Ahmad AlRababah,Digital Image Encryption Implementations Based on AES Algorithm
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>The need for optimal video quality over low delay communication networks is very crucial as their demand increases dramatically. Analysing, evaluating and exploring the codec metrics which can enhance the video quality when transporting the video over low delay communication networks has been a key issue of concern which requires thorough simulation and experimentation. Metrics such as quantization size, frame rate and frame size can significantly enhance the quality video. In this paper, the impacts of frame size, frame rate and quantization parameter on the compressed video quality has been investigated. More extensive simulations have been conducted in order to experiment different scenarios using various video samples at different frame sizes, quantization parameters, loss rate and encoding rate as well. More importantly, both low, medium and high complexity video samples have been examined to determine the impact of the aforementioned parameters on fundamental basic parameters which are essential toward yielding high-quality video while compressing videos. The experimental results show that increase in quantization parameter reduces the bits size and consequently leads to high compression. </div>
		<br>
		</div>","Yakubu Suleiman Baguda, Hani Aljahdali",Video Metrics and Parameters for Achieving Optimal Quality in Low Delay Communication Networks
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>The achievement of the ABET Students Outcomes (SOs) aims to ensure the good preparation of the students to the workplace. The evaluation of the SOs attainment faces many problems including the partial achievement of  SO and the appropriate mapping of the assessment tools to SOs. Furthermore, the complexity and heaviness of the evaluation process require a 2-4 years improvement cycle to avoid faculty overloading. In this work, initially, a class diagram is developed which shows the classes of the system,  their attributes, operations and the relationships among objects</em>. <em>Further</em>, <em>we propose a six-steps methodology that aims to better undertake the assessment of the SOs for an IT program. The first step proceeds to the preparation of an articulation matrix that maps the IT program courses to the SOs. The second step decomposes each SO into its Elementary contents (SOEs) in order to target each SOE separately and contribute to the full achievement of the SO. The detailed articulation matrix is prepared in the third step to include the new list of SOEs. While undertaking the semester-based assessments, the fourth step consists in mapping the assessment tools and questions to the appropriate SOEs using the Bloom's measurable verbs. At the end of the semester, the fifth step is undertaken to input the assessment data for each SOE. The final sixth step proceeds to the analysis of the assessment data to outline important results at both course and program levels. This work is undertaken at the Faculty of Computing and Information Technology in Rabigh (FCITR), King Abdulaziz University, and will be soon supported by a web-based system that helps in the implementation of the six-steps methodology, which will considerably shorten the improvement cycle.</em></p></div>
		<br>
		</div>",Samir K Boucetta,Toward a Better Assessment of Students Outcomes using a Six-Steps method and a Class Diagram
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Now a day bulk of information available on Internet but in spite of will, no one can go through all these information, which may carry huge amount of data. Hence, most of the web surfer required a mechanism through which particular documents or sentences of their interests can be extracted easily without wasting much amount of time. For this purpose, we can generate some keywords out of the stored information, so that keywords requested by the end user should match easily with stored keywords to detect sentences. In this context, Latent Semantic Indexing (LSI) based search method has been applied to whole text and keys. It has been observed through the results that the method with keys (Keywords and key-Sentences) found considerably proficient. Moreover, extracted Keys are not only suited for searching but also appropriate for clustering. Finally, the proposed study not only investigated the detection of keywords but also involved exposure of key sentences</em></div>
		<br>
		</div>","Shakeel Ahmad, Sheikh Muhammad Saqib, Alaa Omran Almagrabi, Fahad Mazaed Alotaibi",LSI Based Search Technique: Using Extracted Keywords and Key-Sentences
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Data security has gauged much attention for over a past few decades. Cryptography is one of the most serious domains as far as security is concerned. This article evaluates and compares among few of the well-known state of art cryptographic algorithm like AES, DES and Serpent depending on the nature of data by different authors in different years. Benchmark techniques (True Positive rate, False Positive rate, ROC curves etc) are used to analyze the sensitivity and specificity for the respective algorithms. Paper is concluded with the best possible algorithm that suits a respective dataset.</em></div>
		<br>
		</div>","Muhammad Usman, Shahid M Awan",A Data Specific Comparative Study for Choosing Best Cryptographic Technique
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>E-Learning and Virtual Classroom applications have gained a lot of popularity due to growing population, easy access and low cost solution. The study in this paper proposes an open source Virtual Classroom application that tends to mimic all the functionalities and features of real class room. Its interface designs are based on the online learning theories. It will provide the students and teachers a real time virtual platform, where they can learn, share and properly propagate their knowledge, views and ideas. This open source application allows the faculty members to conduct all the class activities as if they are in a real classroom. On the other hand, students have the advantage of raising questions during the lecture with the help of a chat box and a whiteboard. In order to assess the interfaces of this application, Microsoft Visual Studio 2012 has been used. Our application provides security and reliability to all its users. All the courses, students and faculty members are managed in a real-time using this application. Administrator handles all these procedures and has all the rights over the system including the users and databases.</em></div>
		<br>
		</div>","Syed Farooq Ali, Hamza Tahir, Zafar Khan",Virtual Classroom  an Easy and Low Cost Solution to E-Learning System
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>In the normal ballot process, voting condition is troublesome in light of dislike of voters to go for voting to polling stations coming from far places, stay in long queues and sit tight drawn out for their turn. A couple of individuals settle on wrong decision which cause star choice issues. This customary strategy for voting can be switched to a more progressive and beneficial approach entitle Mobile-Voting system. We propose wonderful, essential and organized way to deal with vote, shedding the insufficiencies of regular approach. By using the customer id and mystery word against one CNIC, one voter can settle on one decision. If one person has the right to vote then a voting structure is displayed to him and approval is finished by using One Time Password (OTP) rule, thumb impression and face affirmation. Advancement of technology can be used to provide benefit to the people who vote but face difficulties during this process. To give permission of utilization of this right, all over the world where the voting through mobile is conducting have some common steps like voter verification and confirmation, counting and checking of votes, announcement of result. In the proposed system each voter is affirmed by the CNIC, OTP, thumb impression and face affirmation. Each vote caste against a candidate is placed in the database for the individual dispute. At end of the ballot casting system the counter checks the total ballot caste against each candidate and makes a short report of it and give it to the admin. Admin has access to share these results with everyone who has the ballot casting application.</em></div>
		<br>
		</div>","Syed Ali Hassan, Mohsin Anwar",Voting System using Android Operating System
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>In the age of emerging technologies, the amount of data is increasing very rapidly. With the passage of time, the methods of data handling are getting improved. Prediction analysis is quite a tough task, but it also yields interesting results. Different sectors like financial services, transportation, health and education are generating large amount of data. The emergence of web 2.0 (social web) made it possible for users and researchers to analyze and predict huge amount of data. The domain of Business Intelligence is core technology for users who want to extract useful information for decision making regarding their businesses. Data warehouse provides an insight into the business processes using the historical data. However, traditional data warehouse may not be suitable for the data analysis needs because of the evolving requirement of industry. It cannot be scaled up or down. Moreover, it cannot handle the increasing number of users. A new kind of data warehouse with design and implementation aspects has been emerged, called as cloud data warehouse. The cloud data warehouse model has evolved with the passage of time, which affects the application and business domains as well. The cloud data warehouse has evolved to control the large scale data. It can be scaled up or down at any time and also it has no limitation on increasing number of users. In this review paper, we have compared traditional and cloud data warehouse. We can conclude that the ultimate future of data warehouse is cloud data warehouse.</em><em></em></p></div>
		<br>
		</div>","Khawaja Ubaid ur Rehman, Umair Ahmad, Sajid Mahmood",A Comparative Analysis of Traditional and Cloud Data Warehouse
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Interest in research activities in facial processing especially cartoonization, caricature and emotion generation have gradually increased over the recent years. The contribution of this paper is threefold. First, it provides an algorithms along with its results in which exaggerated cartoon like effects are added into a single facial frontal image according to the given cartoon template. The cartoon formed in this process will have similar features as of original image. Secondly, the study provides facial transformation algorithms and techniques to generate various artifacts and emotions including  sad, shy, happy, blank, serious, surprise and innocent. Thirdly, the study discusses different transformation algorithms to generate various caricatures from the single frontal facial image. The output images generated using these open source algorithms and techniques are also provided in this paper to assess their subjective quality.</em></div>
		<br>
		</div>","Syed Farooq Ali, Fahad Zafar, Fahad Zafar, Hafsa Zafar, Hafsa Zafar, Dima Naeem, Dima Naeem, Irum Pervaiz, Irum Pervaiz","Cartoonization, Caricature and Emotion Generation from A Single Intensity Frontal Facial Image"
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>MRI imaging assumes an essential part in brain tumor for conclusion, investigation and treatment arranging. Brain tumor identification is the procedure of the situating of tumor and size. It helps the specialist for deciding the past strides of mind tumor.in this paper, we utilize distinctive methods to obviously distinguish the tumor region from MRI picture. In our approach utilize two level of separating systems these altered half and half middle channel and middle filtering.as the clamor is evacuated we upgrade the picture quality by enhancing dim level of every pixel utilizing KNN mean calculation. The improved picture used to discover the limits of conceivable mind tumor within pictures by identifying discontinuities in the shine. The picture division into a conceivable tumor and non-tumor zones. and afterward sharp both conceivable districts unmistakably envisioned the two regions in brain picture.</em></div>
		<br>
		</div>","Saba Rehman, Iqra Shahzad",Brain Tumor Detection by Using Computer Vision Based on Multi-Level Image Filteration
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>The purpose of this research is to identify the impact of refactoring techniques and design patterns on the source code of the Data management application. The research methodology used for this research is first identify the source code bad smells and then remove them by apply refactoring techniques and design patterns. To evaluate the impact of refactoring and design patterns on the source code quality; the code metrics and visual studio 2015 community edition is used to evaluate code metrics. The results of this research shows that the applying refactoring and design patterns in a combination has a positive effect on the source code quality. The code reusability, expandability and understandability will be increased.</em></div>
		<br>
		</div>","Muhammad Shahid Raees, Muhammad Adeel Ashraf",Effects of Refactoring and Design Patterns on The Software Source Code Quality : An Empirical Assessment
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>kids with Autism Spectrum problems (ASD) have conversation deficits and problems with social interaction. A lack of social conduct can bog down healing interventions and may lessen the potential to examine social talents. Robots had been shown to initiate proactive social conduct in youngsters with ASD. improvement of robot systems able to appearing as catalysts for social conduct in the context of ASD remedy is the need of contemporary era. A methodology and theoretical framework is being supplied here in this paper for a robotic system that can not only compare the effects and stage of ASD, however, can assist the victim consequently.</em></div>
		<br>
		</div>","Danyal Hussain Butt, Wasim Ahmad Khan",Theoretical Framework of Agent Assisted Treatment for ASD (Autism Spectrum Disorder)
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Data cleaning is an action which includes a process of correcting and identifying the inconsistencies and errors in data warehouse. Different terms are uses in these papers like data cleaning also called data scrubbing. Using data scrubbing to get high quality data and this is one the data ETL (extraction transformation and loading tools). Now a day there is a need of authentic information for better decision-making. So we conduct a review paper in which six papers are reviewed related to data cleaning. Relating papers discussed different algorithms, methods, problems, their solutions, and approaches etc. Each paper has their own methods to solve a problem in an efficient way, but all the paper have a common problem of data cleaning and inconsistencies. In these papers data inconsistencies, identification of the errors, conflicting, duplicate records etc problems are discussed in detail and also provided the solutions. These algorithms increase the quality of data. At ETL process stage, there are almost thirty-five different sources and causes of poor quality constraints.</em></div>
		<br>
		</div>","Adeel Ashraf, Sarah Ilyas, Khawaja Ubaid ur Rehman, Shakeel Ahmad",Algorithms for Data Cleaning in Knowledge Bases
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>An important issue among the most vital and essential issues in Wireless Sensor Networks (WSNs) is the area coverage problem. This issue in WSNs causes the security situations directed by the current sensors in the systems suitably. The significance of scope in WSNs is important to the point that is one of the natures of administration parameters. In the event that the sensors don't suitably cover the physical situations they won't be sufficient proficient in supervision and controlling. The scope in WSNs must be in a manner that the vitality of the sensors would be the slightest to build the lifetime of the system. Alternate reasons which had expanded the significance of the issue are the topological changes of the system finished by the harm or cancellation of a percentage of the sensors and now and again the system should not lose its scope. Along these lines, in this paper we have half and half algorithm, the Meta-Heuristic calculations like Differential Evolution and Particle Swarm Optimization algorithms and have broken down the range scope issue in WSNs. Additionally PSO algorithm is executed to look at the productivity of the half and half model in the same circumstances. The consequences of the trials demonstrate that the half and half algorithm has made more increment in the lifetime of the system and more upgraded utilization of the vitality of sensors by improving the scope of the sensors in comparison to PSO.</em></div>
		<br>
		</div>","Asif Farooq, Tahir Iqbal",An Exposition of Wireless Sensor Network Area Coverage and Lifetime Based on Meta Heuristic and Particle Swarm Optimization Algorithms
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>Software Development Life Cycle models are used as the basis to design software applications. In these models, there exists some tribulations and to overcome these defects, agile models are presented. The mature software applications of agile development utilize both iterative and incremental style. It can be viewed as a response against conventional procedural activity. It is viewed not as to meet the volatility and varying situations but as the modern business ethics for software development. By examining and exploring the agile models, it has been found that efficiency of agile models can be improved. This research investigates the agile frameworks (DSDM and LSD) in terms of testing. It also examines that the effectiveness of dynamic systems development model (DSDM) from testing perspective. The results show the performance of DSDM agile framework and the estimated efficiency of this framework.</em></p></div>
		<br>
		</div>","M.Ahmad Nawaz ul Ghani, M. Shoaib Farooq, Amjad Hussain Zahid",Effectiveness of Agile Development Frameworks with Respect to Testing
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Our society has evolved to a threshold where use of machines to automate mundane tasks is constantly increasing in daily life. Providing machines with capability to develop perception from their environment can lead them to perform a great variety of tasks. Facial emotion detection is crucial sub-part of machine perception development. In this article we present a deep learning based approach for Facial emotion Detection. Our model uses a Convolutional Neural Network (CNN) to learn deep features for classification of facial images into one of 22 emotion (Basic 7 + Compound 15) categories considered in this study. We trained our CNN model with the images dataset from Martinez et al. Our Facial Emotion Detection model was developed using keras with theano backend and implemented on a GPU-powered testbed. Our model achieved 67.6% accuracy for basic emotions and 33% accuracy for compound emotions. </em></div>
		<br>
		</div>","Aymun Saif Dar, Sheraz Naseer, Aihtshan Ali, Ishmal Sauf, Muhammad Ahsan",Facial Emotion Detection through Deep Covolutional Neural Networks
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>In the modern World, diseases may occur at any time. Early diagnosis can prevent the serious consequences of the disease. The Computer Aided Diagnosis has very positively influenced the medical field. It helps the Radiologists to diagnose the diseases very quickly, precisely and accurately. The earlier diagnosis can help doctors to cover further spreading of the disease and to overcome at all. In this paper presented the following step to implements the Diagnosis process including the image preprocessing, Feature Extraction, Segmentation and classification. There are different techniques used in Image Segmentation like Fuzzy-C-Mean (FCM) Algorithm, Thresholding, Watershed Clustering Method and Region Growing etc. Feature extraction is the second phase that includes the calculation of different features of segmented lesion. It transforms the data that is in high-dimensional space to some extent of lesser dimensions. This is the final phase the classification phase that which is deals the Measurement of feature that are used the input to support the vector machine in last classify the lesion. This paper works of Computer Aided Diagnosis on liver lesion has briefly described.</em></div>
		<br>
		</div>","Shoaib Farooq, Zoya Khan",A Survey of Computer Aided Diagnosis (Cad) of Liver in Medical Diagnosis
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>The Graphic Processing Units (GPU) are being adopted in many High Processing Computing (HPC) facilities because of their massively parallel and extraordinary computing power, which makes it possible to accelerate many general purpose implementations from different domains. A general-purpose GPU (GPGPU) is a GPU that performs computations that were traditionally handled by central processing unit (CPU) to accelerate applications along with handling traditional computations for graphics rendering. However, GPUs have some limitations, such as increased acquisition costs as well as larger space requirements, more powerful energy supplies, and their utilization is usually low for most workloads. That results in the need of GPU virtualization to maximize the use of an acquired GPU to share between the virtual machines for optimal use, to reduce power utilization and minimize the costs. This study comparatively reviews the recent GPU virtualization techniques including API remoting, para, full and hardware based virtualization, targeted for general-purpose accelerations.</em></div>
		<br>
		</div>","M Alyas, Hamid Hassan",GPGPU Virtualization Techniques a Comparative Survey
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Life is a blessing but some diseases snatch human life away before even they are being diagnosed. One such horrifying disease is cancer. Among cancer, the most leading and common type is breast cancer.  The actual problem lies in the fact that it is very hard and time consuming for even the most experienced medical specialist to detect the disease with high accuracy but the machines and modern computer science techniques have increased the accuracy and reduced the amount of time taken to diagnose cancer. In the subject paper, a new parallel machine learning technique called the two-stage classifier for identifying breast cancer is presented and compared with various existing techniques in terms of accuracy and percentage error reduction. The proposed technique turns out to be better not only in terms of parallelism but also in terms of the evaluated metrics and reduced the error percentage to almost 50% in one of the cases.</em></div>
		<br>
		</div>","Ali Tariq Nagi, Ahmad Wali, Adnan Shahzada, Muhammad Masroor Ahmad",A Parellel two Stage Classifier for Breast Cancer Prediction and Comparison with Various Ensemble Techniques
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>Business process management (BPM) plays a vital role in organizations management. A central piece to that is the collection of business process models. Depending upon the size of the organization, the collection may have many process models in the process repository. A key feature to such a repository is searching of process models which requires computing similarity between a pair of process models. For a given pair of process models, similarity refers to finding whether the two process models that form the pair are similar or not. To compute the similarity between process models, several techniques have been established however a rigorous evaluation of these techniques has either not been conducted on numerous occasions or the evaluation has not been sufficiently rigorous. A key reason to that is the absence of a benchmark set of queries and their relevant process models, as a judge by human experts. In this study, we argue, the fewer queries used for evaluation may not have the necessary diversity to challenge the abilities of the matching techniques. This work is usually not done due to a large number of manual comparisons. It is thus required a pool of queries. A related challenge is to identify a pool of process models that are declared as relevant to the query models. To address these challenges, we have suggested a technique.</em></div>
		<br>
		</div>","Ayesha Asmat, Afnan Iftikhar",Generating Corpus for Evaluating Performance of Process Matching Techniques
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><strong><em>TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) are the most important protocols in complete protocol architecture.  There are many types of attacks that can block the communication or reduce the performance of a protocol. This study provides a detail analysis of TCP and UDP attacks and their application layer protocols. The authors will also suggest that where the security administrator should focus for providing best security. The old datasets such as KDD99 and NSLKDD has many limitations. This study uses UNSW-NB15 dataset which has recently been generated.</em></strong></div>
		<br>
		</div>","Asghar Ali Shah, Yaser Danial Khan, Muhammad Adeel Ashraf",Attacks Analysis of TCP And UDP Of UNCW-NB15 Dataset
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>In this paper, we propose and examine a new relation in </em><em>-Pochhammer symbol. Further, we set up </em><em>-section, </em><em>-factorial and </em><em>-binomial coefficient in term of </em><em>-Pochhammer symbol utilizing our proposed connection.</em></div>
		<br>
		</div>",Aneela Ashraf,NOVEL INVESTIHATIONS IN Q-POCHHAMMER SYMBOL
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>In this paper, we propose and investigate a new relation in q-Pochhammer symbol. Further, we establish q-bracket, q-factorial and q-binomial coefficient in term of q-Pochhammer symbol using our proposed relation. As an application,  we express q-Pochhammer symbols in term of ordinary equations to define new surface graphs.</em></p></div>
		<br>
		</div>","Aneela Ashraf, Muhammad Khalid Mahmood",A NEW RELATION IN Q-POCHHAMMER SYMBOL WITH APPLICATIONS
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>In<em> this paper, I address the problem of the subjectivity classification in text. The subjective text is opinion bearing, whereas the objective text is text without expressing opinions. The supervised learning technique namely, Support Vector Machine (SVM) is used to classify the text as subjective and objective. A publically available dataset of drug reviews is used to conduct the experiments using WEKA platform. The experimental results show that the proposed SVM classifier performed better than the other classifiers</em></div>
		<br>
		</div>",Alaa Omran Almagrabi,Performing Subjectivity Classification in Text Using Support Vector Machine
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>This survey paper explained the different approaches of synchronization of replicas of files placed on distributed systems. The survey tells some older and latest techniques of synchronization. Some techniques are by the interference of metadata servers and some are without any intrusion of MDS. In former technique SS storage servers are used for synchronization among replicas. To maximize the performance, scalability and reliability CEPH is a distributed file system. It makes distinction between meta data and data management by object storage file system run on object file systems. Excellent I/O and metadata management is done on CEPH. Commodity servers and disks are used for multitier distributed systems. Performance reliability, I/O rate, workload in writes operations and less overhead in synchronization are the main focus while synchronization of replicas. Hadoop and Google file system are the distributive file systems. Hadoop ensures the better input and output performance with minimal synchronization in replicas, data intensive applications and provides fault tolerance. Some strategies are used for data intensive applications. Parallel file system is type of distributed file system. Analysis enforces the best performance on small and large input output requests. Pattern direct and layout replication technique is one of the most optimized techniques for parallel file system. Data access performance, reliability, data consistency, centralized synchronization, less workload, less overhead is the main focus of all the techniques. Some other file systems like SOFA and frangipani do focus on data consistency and reduce of bandwidth.</em><em></em></p></div>
		<br>
		</div>","Saher Ather, Hooria Muslin-Ud-Din, Muhammad Nabeel, Muhammad Ahsan, Bilal Hassan",Several Adaptive Replica Synchronization Approaches for Distributed file System
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><p><em>As the research increased in computer science highlight the scientists mind for the growing research world towards security. Researchers have done a lot of research work in network Security. Cybersecurity has progressively become a zone of alarm for officials, Government agencies and industries, including big commercialized infrastructure, are under attack daily. First signature-based intrusion detection systems were developed, and it detects only novel attacks. To detect strange attacks statistical IDS came into being recognized as anomaly-based IDS. It is not as much efficient as it detects all. In this, study the author focus on the efficiency of IDS using NSL-KDD99 dataset and support vector machine (SVM) technique to identify attacks. NSL-KDD dataset is used for the evaluation of these type of systems.</em></p></div>
		<br>
		</div>","Muhammad Hamza Aziz, Asghar Ali Shah",Anomaly Based Intrusion Detection System Which Analyze the Dataset and Detect Intrusion
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div><em>CCTV cameras are commonly used for security issues. Pan-tilt-zoom (PTZ) cameras are mostly used for this purpose. To stitch two or more video streams from different cameras is much cheaper than PTZ solution. There are three stages of video stitching.  Feature identification is the first stage of video stitching. To scale the invariant features like rotation, scaling and noise etc. Direct and feature base identification has basically two types of feature identification. Shifting and warping the images purpose to identify how these features are agreeing with each other is the main concern for direct base identification. While feature identification rely on extracting the features and then perform matching among them on the base of features. Calibration is the second stage of the video stitching. The images are stitch in panoramic way in calibration depending upon alignment among them. Blending is the last stage of video stitching where numerous videos are display in single panoramic way. Any blending algorithm is used to blend the pixels together and for final view. </em></div>
		<br>
		</div>","Ahsan Ali, Asghar Ali Shah, Kashif Aftab, Muhammad Usman",Multicamera Video Stitching Surveillance System
"<div id=""articleAbstract"">
		<h4>Abstract</h4>
		<br>
		<div>This paper describes the accuracy of various algorithms for classification of text on the basis of gender identification. We examined the knowledge extracted from corpus of Twitter's online social media in term of gender identity. By comparing algorithms on different feature sets, we established a feature set of 20 distinct arguments which increase the correctness of gender identification on all over the twitter. We reported accuracies of three algorithms obtained by using two approaches applied on two classes of gender i.e. male and female; a model where a lot of features are reduced using powerset transformation.</div>
		<br>
		</div>","Waqas Ali, Malik Tahir Hassan, Syed Fawad Raza, Usman Fiaz",Classification Of Twitter’s Data To Get Gender Identification
